  **Smart phone based automated diabetic retinopathy detection system**

One of the major causes of blindness across individuals of various age groups and genders is Diabetic Retinopathy (DR). It is mainly caused by Diabetes. The symptoms of DR include seeing an increased number of floaters, blurry vision, vision that changes from blurry to clear sometimes, seeing dark areas in the field of vision, poor night vision and so on. A patient who has been diagnosed with diabetes for a long time is prone to DR. This paper proposes a robust, deep learning based mobile application to address the said issue. The rationale behind the development of the mobile application is to detect DR in far-flung areas, which do not have regular access to healthcare facilities. An input image captured by an ophthalmoscope when fed to the mobile application can give result within a few seconds. The model utilizes convolution neural networks build upon the VGG model. The proposed system quantifies the “confidence level” with a high accuracy of 96 %. Another feature of the proposed system is that it can work offline without any network connectivity, thus making it more useful in remote areas. The application further has a user friendly and intuitive interface.

The model currently being used is a cognate of the existing VGG Architecture, which is widely used for object recognition-based models Dutta et al. The Architecture consists of two variants, one being a 16-layer version and the other, a 19-layer version. For the current use-case, the 16-layer model was chosen because of its uncompromising robustness and the ability to make training quicker than its 19-layer counterpart. VGG on itself is based on an older architecture called AlexNet. VGG differentiates itself by using the smallest receptive field of 3 × 3 pixels which still manages to capture both the axes (Left – Right and Top – Bottom) in an image whereas contrarily, AlexNet used a big receptive field of 11 × 11 pixels Nepal. Furthermore, the stride property of a model, essentially meaning the ability of a model's filter to move about the sample image, is set to 1 pixel in the VGG architecture to 4 in AlexNet. Since VGG has the smallest convolution filters, it gives the user the opportunity to include more weight layers, which in-turn improves the overall performance of the model. Moreover, the trainable parameters in VGG16, a 16-layer version of the VGG architecture, amounts to 138,432,208 whereas the trainable parameters in AlexNet is 62,378,344 Nepal. All the aforesaid features combined makes VGG a faster, more dynamic model when compared to AlexNet.
